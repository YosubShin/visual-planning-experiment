#!/usr/bin/env bash
set -euo pipefail
set -x

log() {
  printf '[setup_env] %s\n' "$*" >&2
}

# Root of the project snapshot (contains repo/ and run_metadata/)
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"

# Shared uv-managed environment (overridable via KOA_SHARED_ENV); lives outside job snapshots
SHARED_ENV_DIR="${KOA_SHARED_ENV:-${PROJECT_ROOT}/../.venv}"
mkdir -p "${SHARED_ENV_DIR}"
ENV_PYTHON="${SHARED_ENV_DIR}/bin/python"

# Add uv to PATH
export PATH="${PATH}:~/.local/bin"

# Cache location for environment hashes (used to detect changes between runs)
ENV_CACHE_DIR="${SHARED_ENV_DIR}/.koa"
ENV_HASH_CACHE="${ENV_CACHE_DIR}/env_hashes.json"

# Locate the manifest generated by `koa submit` (contains env_hashes.json)
RUN_METADATA_DIR="${KOA_RUN_METADATA_DIR:-}"
if [[ -z "${RUN_METADATA_DIR}" && -n "${KOA_RUN_DIR:-}" ]]; then
  RUN_METADATA_DIR="${KOA_RUN_DIR}/run_metadata"
fi
if [[ -z "${RUN_METADATA_DIR}" && -n "${KOA_ML_RESULTS_ROOT:-}" && -n "${SLURM_JOB_ID:-}" ]]; then
  RUN_METADATA_DIR="${KOA_ML_RESULTS_ROOT}/${SLURM_JOB_ID}/run_metadata"
fi

# Load any configured modules (edit these if your project needs different toolchains)
module purge >/dev/null 2>&1 || true
module load lang/Python/3.11.5-GCCcore-13.2.0 >/dev/null 2>&1 || true

# Prefer user-provided CUDA toolchain, fall back to koa_scratch layout
CUDA_HOME="${KOA_CUDA_HOME:-${HOME}/koa_scratch/cuda-12.4}"
if [[ -d "${CUDA_HOME}" ]]; then
  export CUDA_HOME
  export CUDA_PATH="${CUDA_HOME}"
  export PATH="${CUDA_HOME}/bin:${PATH}"
  if [[ -n "${LD_LIBRARY_PATH:-}" ]]; then
    export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"
  else
    export LD_LIBRARY_PATH="${CUDA_HOME}/lib64"
  fi
  log "Configured CUDA toolchain from ${CUDA_HOME}"
else
  log "CUDA toolchain not found at ${CUDA_HOME}; continuing without overriding PATH"
fi


# Prefer python3, fall back to python
python_bin="$(command -v python3 || command -v python)"

if [[ -z "${python_bin}" ]]; then
  log "No python interpreter found" >&2
  exit 1
fi

# Determine whether we need to rebuild or refresh the shared environment
recreate=0
if [[ ! -x "${ENV_PYTHON}" ]]; then
  recreate=1
fi

ENV_HASH_SOURCE=""
if [[ -n "${RUN_METADATA_DIR}" && -f "${RUN_METADATA_DIR}/env_hashes.json" ]]; then
  ENV_HASH_SOURCE="${RUN_METADATA_DIR}/env_hashes.json"
fi

# Compare the last synced env hashes with the current ones; rebuild if they differ
if [[ "${recreate}" -eq 0 && -n "${ENV_HASH_SOURCE}" ]]; then
  mkdir -p "${ENV_CACHE_DIR}"
  if [[ ! -f "${ENV_HASH_CACHE}" ]] || ! cmp -s "${ENV_HASH_SOURCE}" "${ENV_HASH_CACHE}"; then
    recreate=1
  fi
fi

export UV_PROJECT_ENVIRONMENT="${SHARED_ENV_DIR}"
if [[ "${recreate}" -eq 1 ]]; then
  log "Recreating shared environment at ${SHARED_ENV_DIR}"
  # Ensure uv is available for managing the shared environment
  if ! uv --help >/dev/null 2>&1; then
    curl -LsSf https://astral.sh/uv/install.sh | sh
  fi

  # (Re)create the uv-managed environment and install dependencies from this repo snapshot
  uv venv --clear "${SHARED_ENV_DIR}"
  uv sync --extra hpc

  if [[ -n "${ENV_HASH_SOURCE}" ]]; then
    mkdir -p "${ENV_CACHE_DIR}"
    cp "${ENV_HASH_SOURCE}" "${ENV_HASH_CACHE}"
  fi
else
  log "Shared environment already up to date; skipping rebuild"
fi

if [[ ! -x "${ENV_PYTHON}" ]]; then
  log "Shared environment interpreter missing at ${ENV_PYTHON}"
  exit 1
fi

uv run python -c "import torch, flash_attn; print(torch.__version__); print(flash_attn.__version__)"
